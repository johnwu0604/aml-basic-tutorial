{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "We have manually prepared a CIFAR-10 dataset for this tutorial. Start by downloading and unpackaging this dataset. The dataset contains images separated into train, valid, and test folders. Each folder contains 10 subfolders, one for each image class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data.zip \"https://johndatasets.blob.core.windows.net/cifar/data.zip?sp=r&st=2020-02-19T05:25:25Z&se=2021-06-01T12:25:25Z&spr=https&sv=2019-02-02&sr=b&sig=cbcKCgl3r0UmFtUuREb9JOyScMIHIcjn9xXyWo1Gdkw%3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data\n",
    "Now that we have access to the dataset, let's upload it to the datastore that is attached to your workspace. This will allow the data to be mounted to any machine that you use for training. When working with your own data, you can use the sample code below to upload your data from wherever it is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "datastore = workspace.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore.upload('images', target_path='cifar', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Dataset\n",
    "We can register the dataset into Azure Machine Learning, which will tell the system the exact path of where the dataset is stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "cifar_dataset = Dataset.File.from_files(path=(datastore, 'cifar'))\n",
    "\n",
    "cifar_dataset = cifar_dataset.register(workspace=workspace,\n",
    "                                       name='CIFAR-10 Dataset',\n",
    "                                       description='Dataset containing CIFAR 10 type images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also allow anyone in the workspace to easily access the dataset in a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_dataset = workspace.datasets['CIFAR-10 Dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "Now that the dataset is ready, it is time to train our model. We will use the following training script to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat train/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by specifying a name for the experiment. This will create a folder to store the results of each run in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'objection-classification' \n",
    "experiment = Experiment(workspace, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we must define the configuration for the run including conda dependencies, compute environments, training scripts, and script arguments. When training your own custom models, you can modify these configurations based on your custom needs.\n",
    "\n",
    "> **Note**\n",
    "- Our training script uses PyTorch, so we must define the appropriate pip package and version in our dependencies.\n",
    "- We will use our existing compute instance as the compute target. In general, you can use any compute target in the workspace.\n",
    "- All training files must be contained inside the specified source directory.\n",
    "- The data_dir is specified by mounting the dataset that we had registered earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import CondaDependencies\n",
    "from azureml.core import ScriptRunConfig, RunConfiguration\n",
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "\n",
    "# Specify conda packages\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package('torch==1.4.0')\n",
    "conda_dep.add_pip_package('torchvision==0.5.0')\n",
    "conda_dep.add_pip_package('azureml-dataprep[fuse,pandas]==1.4.3')\n",
    "\n",
    "# Specify run config\n",
    "run_config = RunConfiguration()\n",
    "run_config.target = workspace.compute_targets['ds3'] # Change this to the name of your compute instance\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = conda_dep\n",
    "\n",
    "# Specify script run config\n",
    "script_run_config = ScriptRunConfig(source_directory='train', \n",
    "                                    script='train.py',\n",
    "                                    arguments=['--data_dir', cifar_dataset.as_named_input('cifardata').as_mount(),\n",
    "                                               '--output_dir', './outputs', \n",
    "                                               '--num_epochs', 10, \n",
    "                                               '--batch_size', 16,\n",
    "                                               '--learning_rate', 0.001, \n",
    "                                               '--momentum', 0.9],\n",
    "                                    run_config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can submit the run configuration and start training our model. This will build an image using the specified run configurations and deploy a docker container in the specified compute target to run the job. This will ensure the outputs of the run are reproducible in a consistent environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(script_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the status of the run using built-in Jupyter Widgets or visit the **experiments** page in https://ml.azure.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register and Deploy Model\n",
    "\n",
    "TODO: Add descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='cifar-classifier', model_path='outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(source_directory='deploy',\n",
    "                                   runtime='python',\n",
    "                                   entry_script='score.py',\n",
    "                                   conda_file='env.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "from azureml.core import Model\n",
    "\n",
    "local_service_name = 'local-cifar-classifier'\n",
    "local_config = LocalWebservice.deploy_configuration(port=3000)\n",
    "\n",
    "try:\n",
    "    local_service = Webservice(workspace, name=local_service_name)\n",
    "    if local_service:\n",
    "        local_service.delete()\n",
    "except WebserviceException as e:\n",
    "    print()\n",
    "\n",
    "local_service = Model.deploy(workspace, local_service_name, [model], inference_config, local_config)\n",
    "local_service.wait_for_deployment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model\n",
    "TODO: Add descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import urllib.request\n",
    "import base64\n",
    "import requests \n",
    "import json\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def imgToBase64(img):\n",
    "    '''Convert pillow image to base64-encoded image'''\n",
    "    imgio = BytesIO()\n",
    "    img.save(imgio, 'JPEG')\n",
    "    img_str = base64.b64encode(imgio.getvalue())\n",
    "    return img_str.decode('utf-8')\n",
    "\n",
    "def test_service(image_url, scoring_url):\n",
    "    # Download image and convert to base 64\n",
    "    with urllib.request.urlopen(image_url) as url:\n",
    "        test_img = io.BytesIO(url.read())\n",
    "\n",
    "    base64Img = imgToBase64(Image.open(test_img))\n",
    "    \n",
    "    # Get prediciton through endpoint\n",
    "    input_data = '{\\\"data\\\": \\\"'+ base64Img +'\\\"}'\n",
    "    headers = {'Content-Type':'application/json'}\n",
    "    response = requests.post(scoring_url, input_data, headers=headers)\n",
    "    return json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_url = local_service.scoring_uri\n",
    "image_url = 'https://d.newsweek.com/en/full/1528680/delta-airlines.jpg'\n",
    "\n",
    "prediction = test_service(image_url, scoring_url)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
